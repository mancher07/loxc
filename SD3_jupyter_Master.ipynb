{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mancher07/loxc/blob/master/SD3_jupyter_Master.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjYy0F2gZIPR"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch accelerate\n",
        "\n",
        "%cd /content\n",
        "!git clone -b master https://github.com/camenduru/ComfyUI /content/MasterUI\n",
        "%cd /content/MasterUI\n",
        "\n",
        "!pip install torch numpy pillow\n",
        "!pip install -q torchsde einops diffusers accelerate xformers==0.0.26.post1\n",
        "!apt -y install -qq aria2\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/adamo1139/stable-diffusion-3-medium-ungated/resolve/main/sd3_medium_incl_clips_t5xxlfp8.safetensors -d /content/MasterUI/model -o sd3_medium_incl_clips_t5xxlfp8.safetensors\n",
        "\n",
        "import torch\n",
        "import random\n",
        "import node_helpers\n",
        "from comfy.sd import load_checkpoint_guess_config\n",
        "import nodes\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "\n",
        "with torch.inference_mode():\n",
        "    model_patcher, clip, vae, clipvision = load_checkpoint_guess_config(\"/content/MasterUI/model/sd3_medium_incl_clips_t5xxlfp8.safetensors\", output_vae=True, output_clip=True, embedding_directory=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DB-w5x6F9glJ"
      },
      "outputs": [],
      "source": [
        "def zero_out(conditioning):\n",
        "        c = []\n",
        "        for t in conditioning:\n",
        "            d = t[1].copy()\n",
        "            if \"pooled_output\" in d:\n",
        "                d[\"pooled_output\"] = torch.zeros_like(d[\"pooled_output\"])\n",
        "            n = [torch.zeros_like(t[0]), d]\n",
        "            c.append(n)\n",
        "        return (c, )\n",
        "\n",
        "with torch.inference_mode():\n",
        "    latent = {\"samples\": torch.ones([1, 16, 1024 // 8, 1024 // 8]) * 0.0609}\n",
        "    prompt= \"a female character with long, flowing hair that appears to be made of ethereal, swirling patterns resembling the Northern Lights or Aurora Borealis. The background is dominated by deep blues and purples, creating a mysterious and dramatic atmosphere. The character's face is serene, with pale skin and striking features. She wears a dark-colored outfit with subtle patterns. The overall style of the artwork is reminiscent of fantasy or supernatural genres\"\n",
        "    cond, pooled = clip.encode_from_tokens(clip.tokenize(prompt), return_pooled=True)\n",
        "    cond = [[cond, {\"pooled_output\": pooled}]]\n",
        "    negative_prompt = \"bad quality, poor quality, doll, disfigured, jpg, toy, bad anatomy, missing limbs, missing fingers, 3d, cgi\"\n",
        "    n_cond, n_pooled = clip.encode_from_tokens(clip.tokenize(negative_prompt), return_pooled=True)\n",
        "    n_cond = [[n_cond, {\"pooled_output\": n_pooled}]]\n",
        "\n",
        "    n_cond1 = node_helpers.conditioning_set_values(n_cond, {\"start_percent\": 0, \"end_percent\": 0.1})\n",
        "    n_cond2 = zero_out(n_cond)\n",
        "    n_cond2 = node_helpers.conditioning_set_values(n_cond2[0], {\"start_percent\": 0.1, \"end_percent\": 1.0})\n",
        "    n_cond = n_cond1 + n_cond2\n",
        "\n",
        "    seed = 1\n",
        "    if seed == 0:\n",
        "        seed = random.randint(0, 18446744073709551615)\n",
        "    print(seed)\n",
        "    sample = nodes.common_ksampler(model=model_patcher,\n",
        "                            seed=seed,\n",
        "                            steps=28,\n",
        "                            cfg=4.5,\n",
        "                            sampler_name=\"dpmpp_2m\",\n",
        "                            scheduler=\"sgm_uniform\",\n",
        "                            positive=cond,\n",
        "                            negative=n_cond,\n",
        "                            latent=latent,\n",
        "                            denoise=1)\n",
        "    sample = sample[0][\"samples\"].to(torch.float16)\n",
        "    vae.first_stage_model.cuda()\n",
        "    decoded = vae.decode_tiled(sample).detach()\n",
        "\n",
        "Image.fromarray(np.array(decoded*255, dtype=np.uint8)[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt= \"a greeting card for a inaugaration party of a hotel include  a greeting wish \"\n",
        "\n",
        "import torch\n",
        "from transformers import GenerationConfig, GPT2LMHeadModel, GPT2Tokenizer, LogitsProcessor, LogitsProcessorList\n",
        "\n",
        "\n",
        "styles = {\n",
        "    \"cinematic\": \"cinematic film still of {prompt}, highly detailed, high budget hollywood movie, cinemascope, moody, epic, gorgeous, film grain\",\n",
        "    \"anime\": \"anime artwork of {prompt}, anime style, key visual, vibrant, studio anime, highly detailed\",\n",
        "    \"photographic\": \"cinematic photo of {prompt}, 35mm photograph, film, professional, 4k, highly detailed\",\n",
        "    \"comic\": \"comic of {prompt}, graphic illustration, comic art, graphic novel art, vibrant, highly detailed\",\n",
        "    \"lineart\": \"line art drawing {prompt},highly detailed, professional, sleek, modern, minimalist, graphic, line art, vector graphics\",\n",
        "    \"pixelart\": \" pixel-art {prompt}, low-res, blocky, pixel art style, 8-bit graphics\",\n",
        "    \"surrealist\": \"surrealist artwork of {prompt}, dreamlike, bizarre, imaginative, highly detailed, surreal\",\n",
        "    \"steampunk\": \"steampunk illustration of {prompt}, industrial, victorian, gears and cogs , highly detailed\",\n",
        "    \"cartoon\": \"cartoon drawing of {prompt}, playful, colorful, animated style, fun, highly detailed\",\n",
        "    \"impressionist\": \"impressionist painting of {prompt}, soft brush strokes, light and color, artistic, highly detailed\",\n",
        "    \"fantasy\": \"fantasy artwork of {prompt}, magical, ethereal, highly detailed, imaginative, dreamlike\",\n",
        "    \"minimalist\": \"minimalist illustration of {prompt}, clean lines, simple shapes, modern, elegant, highly detailed\",\n",
        "    \"popart\": \"pop art illustration of {prompt}, bold colors, comic book style, high contrast, vibrant, graphic\",\n",
        "    \"abstract\": \"abstract art of {prompt}, vibrant colors, geometric shapes, modern, artistic, highly detailed\",\n",
        "    \"vintage\": \"vintage illustration of {prompt}, retro, nostalgic, sepia tones, highly detailed, classic\",\n",
        "    \"watercolor\": \"watercolor painting of {prompt}, soft colors, dreamy, artistic, hand-painted, highly detailed\",\n",
        "}\n",
        "\n",
        "words = [\n",
        "    \"aesthetic\", \"astonishing\", \"beautiful\", \"breathtaking\", \"composition\", \"contrasted\", \"epic\", \"moody\", \"enhanced\",\n",
        "    \"exceptional\", \"fascinating\", \"flawless\", \"glamorous\", \"glorious\", \"illumination\", \"impressive\", \"improved\",\n",
        "    \"inspirational\", \"magnificent\", \"majestic\", \"hyperrealistic\", \"smooth\", \"sharp\", \"focus\", \"stunning\", \"detailed\",\n",
        "    \"intricate\", \"dramatic\", \"high\", \"quality\", \"perfect\", \"light\", \"ultra\", \"highly\", \"radiant\", \"satisfying\",\n",
        "    \"soothing\", \"sophisticated\", \"stylish\", \"sublime\", \"terrific\", \"touching\", \"timeless\", \"wonderful\", \"unbelievable\",\n",
        "    \"elegant\", \"awesome\", \"amazing\", \"dynamic\", \"trendy\",\n",
        "]\n",
        "\n",
        "word_pairs = [\"highly detailed\", \"high quality\", \"enhanced quality\", \"perfect composition\", \"dynamic light\"]\n",
        "\n",
        "def find_and_order_pairs(s, pairs):\n",
        "    words = s.split()\n",
        "    found_pairs = []\n",
        "    for pair in pairs:\n",
        "        pair_words = pair.split()\n",
        "        if pair_words[0] in words and pair_words[1] in words:\n",
        "            found_pairs.append(pair)\n",
        "            words.remove(pair_words[0])\n",
        "            words.remove(pair_words[1])\n",
        "\n",
        "    for word in words[:]:\n",
        "        for pair in pairs:\n",
        "            if word in pair.split():\n",
        "                words.remove(word)\n",
        "                break\n",
        "    ordered_pairs = \", \".join(found_pairs)\n",
        "    remaining_s = \", \".join(words)\n",
        "    return ordered_pairs, remaining_s\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"Gustavosta/MagicPrompt-Stable-Diffusion\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"Gustavosta/MagicPrompt-Stable-Diffusion\", torch_dtype=torch.float16).to(\n",
        "    \"cuda\"\n",
        ")\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "fsEoVTDjk1yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomLogitsProcessor(LogitsProcessor):\n",
        "    def __init__(self, bias):\n",
        "        super().__init__()\n",
        "        self.bias = bias\n",
        "\n",
        "    def __call__(self, input_ids, scores):\n",
        "        if len(input_ids.shape) == 2:\n",
        "            last_token_id = input_ids[0, -1]\n",
        "            self.bias[last_token_id] = -1e10\n",
        "        return scores + self.bias\n",
        "\n",
        "word_ids = [tokenizer.encode(word, add_prefix_space=True)[0] for word in words]\n",
        "bias = torch.full((tokenizer.vocab_size,), -float(\"Inf\")).to(\"cuda\")\n",
        "bias[word_ids] = 0\n",
        "processor = CustomLogitsProcessor(bias)\n",
        "processor_list = LogitsProcessorList([processor])\n",
        "\n",
        "style = \"surrealist\"\n",
        "\n",
        "prompt = styles[style].format(prompt=prompt)\n",
        "prompt"
      ],
      "metadata": {
        "id": "zKiW9hMilnQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"Gustavosta/MagicPrompt-Stable-Diffusion\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"Gustavosta/MagicPrompt-Stable-Diffusion\", torch_dtype=torch.float16).to(\n",
        "    \"cuda\"\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "token_count = inputs[\"input_ids\"].shape[1]\n",
        "max_new_tokens = 50 - token_count\n",
        "\n",
        "generation_config = GenerationConfig(\n",
        "    penalty_alpha=0.7,\n",
        "    top_k=50,\n",
        "    eos_token_id=model.config.eos_token_id,\n",
        "    pad_token_id=model.config.eos_token_id,\n",
        "    pad_token=model.config.pad_token_id,\n",
        "    do_sample=True,\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        generation_config=generation_config,\n",
        "        logits_processor=processor_list,\n",
        "    )\n",
        "\n",
        "output_tokens = [tokenizer.decode(generated_id, skip_special_tokens=True) for generated_id in generated_ids]\n",
        "input_part, generated_part = output_tokens[0][: len(prompt)], output_tokens[0][len(prompt) :]\n",
        "pairs, words = find_and_order_pairs(generated_part, word_pairs)\n",
        "formatted_generated_part = pairs + \", \" + words\n",
        "enhanced_prompt = input_part + \", \" + formatted_generated_part\n",
        "enhanced_prompt"
      ],
      "metadata": {
        "id": "K5bkTr8Al6RA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initial_prompt = \"A serene landscape with mountains and a lake at sunset.\"\n",
        "model_name = \"gpt2\"  # You can use other models available on Hugging Face\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "def generate_improved_prompt(initial_prompt, model_name=\"gpt2\", max_length=50):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "    input_ids = tokenizer.encode(f\"Improve the following image generation prompt: '{initial_prompt}'\", return_tensors=\"pt\")\n",
        "    output = model.generate(input_ids, max_length=max_length, num_return_sequences=1, no_repeat_ngram_size=2)\n",
        "\n",
        "    improved_prompt = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return improved_prompt\n",
        "\n",
        "# Generate an improved prompt using the function\n",
        "improved_prompt = generate_improved_prompt(prompt, model_name)\n",
        "print(\"Improved prompt:\", improved_prompt)"
      ],
      "metadata": {
        "id": "dqGC-tA8mCzO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}